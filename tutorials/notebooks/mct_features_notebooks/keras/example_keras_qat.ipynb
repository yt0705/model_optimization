{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f31596-c293-4faa-8253-336769f8faa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quantization Aware Training using the Model Compression Toolkit - example in Keras\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_qat.ipynb)\n",
    "## Overview\n",
    "This tutorial will demonstrate how to use the Quantization Aware Training (QAT) API of the Model Compression Toolkit (MCT). We will train a neural network on the MNIST dataset and apply quantization using the MCT QAT API to optimize the model for efficient hardware deployment without sacrificing accuracy.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. **Training a Keras model on MNIST:** We'll begin by constructing a simple neural network and training it on the MNIST dataset. \n",
    "2. **Configuring Target Platform Capabilities (TPC):** Define the quantization settings for weights and activations.\n",
    "3. **Preparing the Model for QAT:** Convert the floating-point model into a QAT-ready model using MCT. \n",
    "4. **Training the Model with QAT:**  Perform quantization-aware training to preserve model accuracy.\n",
    "5. **Evaluating and Exporting the Quantized Model:** Finalize and export the optimized quantized model for deployment.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380c492-3c53-4ec1-987e-de693a1ec1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "!pip install -q tensorflow~={TF_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee8ecbde24b55fbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c27b1-65f9-4fd3-be3e-733f4c60124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Model, layers, datasets\n",
    "import model_compression_toolkit as mct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Preprocessing MNIST\n",
    "Let's define the dataset loaders to retrieve the train and test parts of the MNIST dataset, including preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2c524e5d4985e47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Add Channels axis to data\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "729d91394f1ded4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Keras Model\n",
    "In this section, we create a simple Keras model to demonstrate the QAT process. The model consists of two convolutional layers, two dense layers, and dropout layers for regularization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb31c629993369f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    _input = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, 3, strides=2, padding='same', activation='relu')(_input)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=_input, outputs=x)\n",
    "    model.summary()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c6f97097cbf81f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model on MNIST\n",
    "Next, we will train the dense model using the preprocessed MNIST dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a31cff183889d31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 128\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n",
    "model.evaluate(test_images, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e6f320b23217c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Model for Hardware-Friendly Quantization Aware Training with MCT\n",
    "## Target Platform Capabilities\n",
    "MCT optimizes the model for dedicated hardware. This is done using TPC (for more details, please visit our [documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/modules/target_platform_capabilities.html)). In this tutorial, we use a TPC configuration that applies 2-bit quantization for weights and 3-bit quantization for activations.\n",
    "\n",
    "If desired, you can skip this step and directly use the pre-configured [`get_target_platform_capabilities`](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/methods/get_target_platform_capabilities.html) function to obtain an initialized TPC."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f7e6ec541426aa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6f84b-9775-4989-9f74-688958f3a1d3",
   "metadata": {
    "id": "8bb6f84b-9775-4989-9f74-688958f3a1d3"
   },
   "outputs": [],
   "source": [
    "from mct_quantizers import QuantizationMethod\n",
    "from model_compression_toolkit.constants import FLOAT_BITWIDTH\n",
    "from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, BIAS_ATTR\n",
    "from model_compression_toolkit.target_platform_capabilities.schema.mct_current_schema import schema\n",
    "\n",
    "def get_tpc():\n",
    "    \"\"\"\n",
    "    Assuming a target hardware that uses a power-of-2 threshold for activations and\n",
    "    a symmetric threshold for the weights. The activations are quantized to 3 bits, and the kernel weights\n",
    "    are quantized to 2 bits. Our assumed hardware does not require quantization of some layers\n",
    "    (e.g. Flatten & Droupout).\n",
    "    This function generates a FrameworkQuantizationCapabilities with the above specification.\n",
    "\n",
    "    Returns:\n",
    "         FrameworkQuantizationCapabilities object\n",
    "    \"\"\"\n",
    "\n",
    "    # define a default quantization config for all non-specified weights attributes.\n",
    "    default_weight_attr_config = schema.AttributeQuantizationConfig(\n",
    "        weights_quantization_method=QuantizationMethod.POWER_OF_TWO,\n",
    "        weights_n_bits=8,\n",
    "        weights_per_channel_threshold=False,\n",
    "        enable_weights_quantization=False,\n",
    "        lut_values_bitwidth=None)\n",
    "\n",
    "    # define a quantization config to quantize the kernel (for layers where there is a kernel attribute).\n",
    "    kernel_base_config = schema.AttributeQuantizationConfig(\n",
    "        weights_quantization_method=QuantizationMethod.SYMMETRIC,\n",
    "        weights_n_bits=2,\n",
    "        weights_per_channel_threshold=True,\n",
    "        enable_weights_quantization=True,\n",
    "        lut_values_bitwidth=None)\n",
    "\n",
    "    # define a quantization config to quantize the bias (for layers where there is a bias attribute).\n",
    "    bias_config = schema.AttributeQuantizationConfig(\n",
    "        weights_quantization_method=QuantizationMethod.POWER_OF_TWO,\n",
    "        weights_n_bits=FLOAT_BITWIDTH,\n",
    "        weights_per_channel_threshold=False,\n",
    "        enable_weights_quantization=False,\n",
    "        lut_values_bitwidth=None)\n",
    "\n",
    "    # Create a default OpQuantizationConfig where we use default_weight_attr_config as the default\n",
    "    # AttributeQuantizationConfig for weights with no specific AttributeQuantizationConfig.\n",
    "    # MCT will compress a layer's kernel and bias according to the configurations that are\n",
    "    # set in KERNEL_ATTR and BIAS_ATTR that are passed in attr_weights_configs_mapping.\n",
    "    default_config = schema.OpQuantizationConfig(\n",
    "        default_weight_attr_config=default_weight_attr_config,\n",
    "        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config,\n",
    "                                      BIAS_ATTR: bias_config},\n",
    "        activation_quantization_method=QuantizationMethod.POWER_OF_TWO,\n",
    "        activation_n_bits=3,\n",
    "        supported_input_activation_n_bits=8,\n",
    "        enable_activation_quantization=True,\n",
    "        quantization_preserving=False,\n",
    "        fixed_scale=None,\n",
    "        fixed_zero_point=None,\n",
    "        simd_size=None,\n",
    "        signedness=schema.Signedness.AUTO)\n",
    "\n",
    "    # Set default QuantizationConfigOptions in new TargetPlatformCapabilities to be used when no other\n",
    "    # QuantizationConfigOptions is set for an OperatorsSet.\n",
    "    default_configuration_options = schema.QuantizationConfigOptions(quantization_configurations=[default_config])\n",
    "    no_quantization_config = (default_configuration_options.clone_and_edit(enable_activation_quantization=False)\n",
    "                              .clone_and_edit_weight_attribute(enable_weights_quantization=False))\n",
    "\n",
    "    operator_set = []\n",
    "\n",
    "    operator_set.append(schema.OperatorsSet(name=schema.OperatorSetNames.DROPOUT, qc_options=no_quantization_config))\n",
    "    operator_set.append(schema.OperatorsSet(name=schema.OperatorSetNames.FLATTEN, qc_options=no_quantization_config))\n",
    "\n",
    "\n",
    "    tpc = schema.TargetPlatformCapabilities(default_qco=default_configuration_options,\n",
    "                                            tpc_minor_version=1,\n",
    "                                            tpc_patch_version=0,\n",
    "                                            tpc_platform_type=\"custom_qat_notebook_tpc\",\n",
    "                                            operator_set=tuple(operator_set))\n",
    "    return tpc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a73bebf51aaf672c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "\n",
    "def representative_data_gen():\n",
    "    def _generator():\n",
    "        for _ind in range(n_iter):\n",
    "            yield [train_images[_ind][np.newaxis, ...]]\n",
    "    return _generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a82f6b9bae8269a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a QAT-Ready Model with MCT\n",
    "The MCT converts a floating-point model into a quantized model using post-training quantization. The returned model includes trainable quantizers and is ready for fine-tuning, making it a \"QAT-ready\" model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4a247d3bde88990"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c171d2d-6f0d-474d-aab6-22b0b0c9e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model, _, custom_objects = mct.qat.keras_quantization_aware_training_init_experimental(\n",
    "    model,\n",
    "    representative_data_gen(),\n",
    "    target_platform_capabilities=get_tpc())\n",
    "qat_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets evaluate the performance after the basic post-trainig quantization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38ba814d794cd57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = qat_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"PTQ model test accuracy: {score[1]:02.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "976a5942ac5fa9c6"
  },
  {
   "cell_type": "markdown",
   "id": "78c74675-b4b5-42bd-a0b7-75da240cbf66",
   "metadata": {},
   "source": [
    "## User Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f1d23-9610-415a-84c2-8ef953370574",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "score = qat_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"QAT model test accuracy: {score[1]:02.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finalizing the QAT model: \n",
    "Remove the 'QuantizeWrapper' layers to retain only the layers with quantized weights (FakeQuant values)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d34761c03296a31a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856de5a6-29d6-4e65-80b1-9f11ed63ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = mct.qat.keras_quantization_aware_training_finalize_experimental(qat_model)\n",
    "\n",
    "quantized_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "score = quantized_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"Quantized model test accuracy: {score[1]:02.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can export the quantized model to Keras:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52158ce9c8ded4bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856de5a6-29d6-4e65-80b1-9f11ed63ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "In this tutorial, we explored how to perform Quantization Aware Training (QAT) using the Model Compression Toolkit (MCT) with a Keras model. We began by constructing a simple neural network and preparing it for quantization by configuring the Target Platform Capabilities (TPC). Then, we converted the model into a QAT-ready format and demonstrated how to train and fine-tune it using hardware-friendly quantization settings. This approach can significantly reduce the model size and improve inference speed while maintaining high accuracy, making it ideal for edge AI applications.\n",
    "\n",
    "Feel free to experiment with different configurations to see how they impact your models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c4089ae72fb2c6d"
  },
  {
   "cell_type": "markdown",
   "id": "db77d678-1fa7-4dc0-a6f3-bac10ba2d8ed",
   "metadata": {},
   "source": [
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
