{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# A Practical Guide to Activation Threshold Search in Post-Training Quantization\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_activation_threshold_search.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to find the optimal activation threshold, a key component in MCT's post-training quantization workflow.\n",
    "\n",
    "In this example, we will explore two different metrics for threshold selection. We will begin by applying the appropriate MCT configurations, followed by inferring a representative dataset through the model. Next, we will plot the activation distributions of two layers along with their corresponding MCT-calculated thresholds, and finally, we will compare the quantized model accuracy using both methods.\n",
    "\n",
    "## Activation threshold explanation\n",
    "During the quantization process, thresholds are used to map a distribution of 32-bit floating-point values to their quantized equivalents. Minimizing data loss while preserving the most representative range is crucial for maintaining the final model's accuracy.\n",
    "\n",
    "### How Is It Done in MCT?\n",
    "\n",
    "MCT's post-training quantization leverages a representative dataset to evaluate a range of typical output activation values. The challenge lies in determining the best way to map these values to their quantized versions. To address this, a grid search is performed to find the optimal threshold using various error metrics. Typically, mean squared error (MSE) is the most effective and is used as the default metric.\n",
    "\n",
    "The error is calculated based on the difference between the original float and the quantized distributions. The optimal threshold is then selected based on the metric that results in the minimum error. For example, for the case of MSE.\n",
    "\n",
    "$$\n",
    "ERR(t) = \\frac{1}{n_s} \\sum_{X \\in Fl(D)} (Q(X, t, n_b) - X)^2\n",
    "$$\n",
    "\n",
    "- $ERR(t)$ : The quantization error function dependent on the threshold $t$.\n",
    "- \n",
    "- $n_s$: The size of the representative dataset.\n",
    "\n",
    "- $\\sum$: Summation over all elements $X$ in the flattened dataset $F_l(D)$.\n",
    "\n",
    "- $F_l(D)$: The set of activation tensors in the $l$-th layer, flattened for processing.\n",
    "\n",
    "- $Q(X, t, n_b)$: The quantized approximation of $X$, given a threshold $t$ and bit width $n_b$.\n",
    "\n",
    "- $X$: The original activation tensor before quantization.\n",
    "\n",
    "- $t$: The quantization threshold, a key parameter for controlling the quantization process.\n",
    "\n",
    "- $n_b$: The number of bits used in quantization, impacting model precision and size.\n",
    "\n",
    "\n",
    "Quantization thresholds often have specific limitations, typically imposed for deployment purposes. In MCT, activation thresholds are restricted by default to Power-of-Two values and can represent either signed values within the range $(-T, T)$ or unsigned values within $(0, T)$. Other restriction settings are also configurable.\n",
    "\n",
    "### Error methods supported by MCT:\n",
    "\n",
    "- **NOCLIPPING:** Use min/max values as thresholds.\n",
    "\n",
    "- **MSE:** Minimizes quantization noise by using the mean squared error (MSE).\n",
    "\n",
    "- **MAE:** Minimizes quantization noise by using the mean absolute error (MAE).\n",
    "\n",
    "- **KL:** Uses Kullback-Leibler (KL) divergence to align the distributions, ensuring that the quantized distribution is as similar as possible to the original.\n",
    "\n",
    "- **Lp:** Minimizes quantization noise using the Lp norm, where `p` is a configurable parameter that determines the type of distance metric.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "!pip install -q tensorflow~={TF_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7837babf2112542b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d691159f5bfc53e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "float_model = MobileNetV2()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "468d67cd5f25886e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation\n",
    "### Download the ImageNet validation set\n",
    "Download the ImageNet dataset with only the validation split.\n",
    "**Note:** For demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de5a1be0c4fc4847"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_ztv72uM6-UT",
   "metadata": {
    "id": "_ztv72uM6-UT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    \n",
    "    !cd imagenet && tar -xzf ILSVRC2012_devkit_t12.tar.gz && \\\n",
    "     mkdir ILSVRC2012_img_val && tar -xf ILSVRC2012_img_val.tar -C ILSVRC2012_img_val"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code organizes the extracted data into separate folders for each label, making it compatible with Keras dataset loaders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca398ea7e1551d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YVAoUjK47Zcp",
   "metadata": {
    "id": "YVAoUjK47Zcp"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "root = Path('./imagenet')\n",
    "imgs_dir = root / 'ILSVRC2012_img_val'\n",
    "target_dir = root /'val'\n",
    "\n",
    "def extract_labels():\n",
    "    !pip install -q scipy\n",
    "    import scipy\n",
    "    mat = scipy.io.loadmat(root / 'ILSVRC2012_devkit_t12/data/meta.mat', squeeze_me=True)\n",
    "    cls_to_nid = {s[0]: s[1] for i, s in enumerate(mat['synsets']) if s[4] == 0} \n",
    "    with open(root / 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt', 'r') as f:\n",
    "        return [cls_to_nid[int(cls)] for cls in f.readlines()]\n",
    "\n",
    "if not target_dir.exists():\n",
    "    labels = extract_labels()\n",
    "    for lbl in set(labels):\n",
    "        os.makedirs(target_dir / lbl)\n",
    "    \n",
    "    for img_file, lbl in zip(sorted(os.listdir(imgs_dir)), labels):\n",
    "        shutil.move(imgs_dir / img_file, target_dir / lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "These functions generate a `tf.data.Dataset` from image files in a directory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0bb1a9df8e1d7fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels\n",
    "\n",
    "def get_dataset(batch_size, shuffle):\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory='./imagenet/val',\n",
    "        batch_size=batch_size,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=shuffle,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8acd6413a722c2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8aa0bca3e15fba91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_iter = 10\n",
    "\n",
    "dataset = get_dataset(batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(n_iter):\n",
    "        yield [dataset.take(1).get_single_element()[0].numpy()]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bdb4144e4ce2ab6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Platform Capabilities\n",
    "MCT optimizes the model for dedicated hardware. This is done using TPC (for more details, please visit our [documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/modules/target_platform_capabilities.html)). Here, we use the default Tensorflow TPC:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98f4bbca00996989"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a FrameworkQuantizationCapabilities object that models the hardware for the quantized model inference. Here, for example, we use the default platform that is attached to a Keras layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('tensorflow', 'default')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554719effaf90250"
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## Post-Training Quantization using MCT\n",
    "In this step, we load the model and apply post-training quantization using two threshold error calculation methods: **\"No Clipping\"** and **MSE**.\n",
    "\n",
    "- **\"No Clipping\"** selects the lowest power-of-two threshold that ensures no data is lost (clipped).\n",
    "- **MSE** selects a power-of-two threshold that minimizes the mean square error between the original float distribution and the quantized distribution.\n",
    "\n",
    "- As a result, the \"No Clipping\" method typically results in a larger threshold, as we will demonstrate later in this tutorial.\n",
    "\n",
    "The quantization parameters are predefined, and we use the default values except for the quantization method. Feel free to modify the code below to experiment with other error metrics supported by MCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jtiZzXmTjxuI",
   "metadata": {
    "id": "jtiZzXmTjxuI"
   },
   "outputs": [],
   "source": [
    "from model_compression_toolkit.core import QuantizationErrorMethod\n",
    "\n",
    "q_configs_dict = {}\n",
    "# Error methods to iterate over\n",
    "error_methods = [\n",
    "    QuantizationErrorMethod.MSE,\n",
    "    QuantizationErrorMethod.NOCLIPPING\n",
    "]\n",
    "\n",
    "# If you are curious you can add any of the below quantization methods as well.\n",
    "# QuantizationErrorMethod.MAE\n",
    "# QuantizationErrorMethod.KL\n",
    "# QuantizationErrorMethod.LP\n",
    "\n",
    "# Iterate and build the QuantizationConfig objects\n",
    "for error_method in error_methods:\n",
    "    q_config = mct.core.QuantizationConfig(\n",
    "        activation_error_method=error_method,\n",
    "    )\n",
    "\n",
    "    q_configs_dict[error_method] = q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8W3Dcn0jkJOH",
   "metadata": {
    "id": "8W3Dcn0jkJOH"
   },
   "source": [
    "Now we will run post-training quantization for each configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c6e55-d474-4dc3-9a43-44b736635998",
   "metadata": {
    "id": "ba0c6e55-d474-4dc3-9a43-44b736635998"
   },
   "outputs": [],
   "source": [
    "quantized_models_dict = {}\n",
    "\n",
    "for error_method, q_config in q_configs_dict.items():\n",
    "    # Create a CoreConfig object with the current quantization configuration\n",
    "    ptq_config = mct.core.CoreConfig(quantization_config=q_config)\n",
    "\n",
    "    # Perform MCT post-training quantization\n",
    "    quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "        in_model=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        core_config=ptq_config,\n",
    "        target_platform_capabilities=target_platform_cap\n",
    "    )\n",
    "\n",
    "    # Update the dictionary to include the quantized model\n",
    "    quantized_models_dict[error_method] = {\n",
    "        \"quantization_config\": q_config,\n",
    "        \"quantized_model\": quantized_model,\n",
    "        \"quantization_info\": quantization_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8UHRsh2khM4",
   "metadata": {
    "id": "A8UHRsh2khM4"
   },
   "source": [
    "## Threshold and Distribution Visualization\n",
    "To facilitate understanding, we will plot the activation distributions for two layers of MobileNetV2. For each layer, we will show the thresholds determined by both **MSE** and **No Clipping** methods, along with the corresponding activation distributions obtained by infering the representative dataset through the model. This visualization highlights the trade-off between data loss and data resolution under different thresholds during quantization.\n",
    "\n",
    "MCT’s `quantization_info` stores the threshold values for each layer. However, to view the actual activation distributions, the model needs to be reconstructed up to and including the target layer selected for visualization.\n",
    "\n",
    "To do this, we first need to identify the layer names. In Keras, this can be easily done for the first 10 layers using the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac",
   "metadata": {
    "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac"
   },
   "outputs": [],
   "source": [
    "for index, layer in enumerate(float_model.layers):\n",
    "    if index < 10:\n",
    "        print(layer.name)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277",
   "metadata": {
    "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277"
   },
   "source": [
    "The first activation layer in the model is named `Conv1_relu`.\n",
    "\n",
    "For this particular model, testing has shown that the `expanded_conv_project_BN` layer exhibits different thresholds for the two error metrics. Therefore, we will also include this layer in the visualization. For context, MobileNetV2 uses an inverted residual structure, where the input is first expanded in the channel dimension, then passed through a depthwise convolution, and finally projected back to a lower dimension. The `expanded_conv_project_BN` layer represents this projection, and the BN suffix indicates the presence of Batch Normalization.\n",
    "\n",
    "We will use these layer names to create two separate models, each ending at one of these respective layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d",
   "metadata": {
    "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "layer_name1 = 'Conv1_relu'\n",
    "layer_name2 = 'expanded_conv_project_BN'\n",
    "\n",
    "layer_output1 = float_model.get_layer(layer_name1).output\n",
    "activation_model_relu = Model(inputs=float_model.input, outputs=layer_output1)\n",
    "layer_output2 = float_model.get_layer(layer_name2).output\n",
    "activation_model_project = Model(inputs=float_model.input, outputs=layer_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364",
   "metadata": {
    "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364"
   },
   "source": [
    "Infer the representative dataset using these models and store the outputs for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9888-5d67-4979-af50-80781a811b4b",
   "metadata": {
    "id": "eaeb9888-5d67-4979-af50-80781a811b4b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "activation_batches_relu = []\n",
    "activation_batches_project = []\n",
    "for images in representative_dataset_gen():\n",
    "    activations_relu = activation_model_relu.predict(images)\n",
    "    activation_batches_relu.append(activations_relu)\n",
    "    activations_project = activation_model_project.predict(images)\n",
    "    activation_batches_project.append(activations_project)\n",
    "\n",
    "all_activations_relu = np.concatenate(activation_batches_relu, axis=0).flatten()\n",
    "all_activations_project = np.concatenate(activation_batches_project, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I5W9yY5DvOFr",
   "metadata": {
    "id": "I5W9yY5DvOFr"
   },
   "source": [
    "Thresholds calculated by MCT during quantization can be accessed using the following approach. The layer indices correspond to the order of the layers listed in the previous steps.\n",
    "\n",
    "As noted earlier, we focus on the first ReLU activation layer and the Batch Normalization layer (`expanded_conv_project_BN`) since they effectively illustrate the impact of the two threshold error methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NGnjrPD_uTd5",
   "metadata": {
    "id": "NGnjrPD_uTd5"
   },
   "outputs": [],
   "source": [
    "# layer 4 is the first activation layer - Conv1_relu\n",
    "layer_name2 = 'expanded_conv_project_BN'\n",
    "optimal_thresholds_relu = {\n",
    "    error_method: data[\"quantized_model\"].layers[4].activation_holder_quantizer.get_config()['threshold'][0]\n",
    "    for error_method, data in quantized_models_dict.items()\n",
    "}\n",
    "\n",
    "# layer 9 is the batch normalisation projection layer - Expanded_conv_project_BN\n",
    "optimal_thresholds_project = {\n",
    "    error_method: data[\"quantized_model\"].layers[9].activation_holder_quantizer.get_config()['threshold'][0]\n",
    "    for error_method, data in quantized_models_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRAr8L5mvuLd",
   "metadata": {
    "id": "XRAr8L5mvuLd"
   },
   "source": [
    "### Distribution Plots\n",
    "Below are the activation distributions for the two selected layers: first, the ReLU activation layer, `Conv1_relu`, followed by the `expanded_conv_project_BN` layer.\n",
    "\n",
    "The second distribution clearly highlights the differences between the two error metrics, showing the impact of each on the resulting quantization threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VPb8tBNGpJjo",
   "metadata": {
    "id": "VPb8tBNGpJjo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_relu, bins=100, alpha=0.5, label='Original')\n",
    "for method, threshold in optimal_thresholds_relu.items():\n",
    "    plt.axvline(threshold, linestyle='--', linewidth=2, label=f'{method}: {threshold:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds First Relu Layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Df7eKzh4oj5X",
   "metadata": {
    "id": "Df7eKzh4oj5X"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_project, bins=100, alpha=0.5, label='Original')\n",
    "for method, threshold in optimal_thresholds_project.items():\n",
    "    plt.axvline(threshold, linestyle='--', linewidth=2, label=f'{method}: {threshold:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds Project BN layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c967d41-439d-405b-815f-be641f1768fe",
   "metadata": {
    "id": "4c967d41-439d-405b-815f-be641f1768fe"
   },
   "source": [
    "## Model Evaluation\n",
    "Finally, we can demonstrate the impact of these different thresholds on the model's overall accuracy.\n",
    "In order to evaluate our models, we first need to load the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataset = get_dataset(batch_size=50, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9199b59c4f10eca1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=\"accuracy\")\n",
    "float_accuracy = float_model.evaluate(val_dataset)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy[1] * 100):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "631780a79e2cedf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613",
   "metadata": {
    "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613"
   },
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "    quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "\n",
    "    results = quantized_model.evaluate(val_dataset, verbose=0)  # Set verbose=0 to suppress the log messages\n",
    "\n",
    "    evaluation_results[error_method] = results\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Results for {error_method}: Loss = {results[0]}, Accuracy = {results[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GpEZ2E1qzWl3",
   "metadata": {
    "id": "GpEZ2E1qzWl3"
   },
   "source": [
    "These results are consistent across many models, which is why MSE is set as the default method.\n",
    "\n",
    "Each of MCT's error methods impacts models differently, so it is recommended to include this metric as part of hyperparameter tuning when optimizing quantized model accuracy.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial, we explored the process of finding optimal activation thresholds using different error metrics in MCT’s post-training quantization workflow. By comparing the **MSE** and **No Clipping** methods, we demonstrated how the choice of threshold can significantly affect the activation distributions and, ultimately, the quantized model’s performance. While **MSE** is commonly the best choice and is used by default, it is essential to consider other error metrics during hyperparameter tuning to achieve the best results for different models.\n",
    "\n",
    "Understanding the impact of these thresholds on data loss and resolution is critical when fine-tuning the quantization process for deployment, making this a valuable step in building high-performance quantized models.\n",
    "\n",
    "\n",
    "## Appendix\n",
    "Below is a code snippet that can be used to extract information from each layer in the MCT quantization output, assisting in analyzing the layer-wise quantization details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qml4LLmWZLP4",
   "metadata": {
    "id": "qml4LLmWZLP4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "quantized_model = data[\"quantized_model\"]\n",
    "quantizer_object = quantized_model.layers[1]\n",
    "\n",
    "quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "\n",
    "relu_layer_indices = []\n",
    "\n",
    "\n",
    "for i, layer in enumerate(quantized_model.layers):\n",
    "    # Convert the layer's configuration to a string\n",
    "    layer_config_str = str(layer.get_config())\n",
    "\n",
    "    layer_class_str = str(layer.__class__.__name__)\n",
    "\n",
    "    # Check if \"relu\" is mentioned in the layer's configuration or class name\n",
    "    if 'relu' in layer_config_str.lower() or 'relu' in layer_class_str.lower():\n",
    "        relu_layer_indices.append(i)\n",
    "\n",
    "print(\"Layer indices potentially using ReLU:\", relu_layer_indices)\n",
    "print(\"Number of relu layers \" + str(len(relu_layer_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207",
   "metadata": {
    "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207"
   },
   "outputs": [],
   "source": [
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "    print(quantized_model.layers[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
