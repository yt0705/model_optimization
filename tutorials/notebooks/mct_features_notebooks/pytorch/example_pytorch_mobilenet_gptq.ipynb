{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Gradient-Based Post Training Quantization using the Model Compression Toolkit - A Quick-Start Guide\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_mobilenet_gptq.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates a pre-trained model quantization using the **Model Compression Toolkit (MCT)** with **Gradient-based PTQ (GPTQ)**. \n",
    "GPTQ stands as an optimization procedure that markedly enhances the performance of models undergoing post-training quantization.\n",
    "This is achieved through an optimization process applied post-quantization, specifically adjusting the rounding of quantized weights.\n",
    "GPTQ is especially effective in case of low bit width quantization and mixed precision quantization.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial we will cover:\n",
    "\n",
    "1. Gradient-Based Post-Training Quantization using MCT.\n",
    "2. Loading and preprocessing ImageNet's validation dataset.\n",
    "3. Constructing an unlabeled representative dataset.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Install and import the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision onnx\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6db5eec3f181f263"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff4ee7d2209752a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46435ae8d8957b72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization and GPTQ optimization. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c18b26e293b085e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36529e754226cb00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9611251c15034c9"
  },
  {
   "cell_type": "markdown",
   "id": "028112db-3143-4fcb-96ae-e639e6476c31",
   "metadata": {
    "id": "028112db-3143-4fcb-96ae-e639e6476c31"
   },
   "source": [
    "### Representative Dataset\n",
    "\n",
    "GPTQ is a gradient-based optimization process, which requires representative dataset to perform inference and compute gradients. \n",
    "\n",
    "Separate representative datasets can be used for the PTQ statistics collection and for GPTQ. In this tutorial we use the same representative dataset for both.\n",
    "\n",
    "A complete pass through the representative dataset generator constitutes an epoch (batch_size x n_iter samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57",
   "metadata": {
    "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9",
   "metadata": {
    "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9"
   },
   "source": [
    "## Model Gradient-Based Post-Training quantization using MCT\n",
    "\n",
    "This is the main part in which we quantize and our model.\n",
    "\n",
    "Next, we create a **GPTQ configuration** with possible GPTQ optimization options (such as the number of epochs for the optimization process). \n",
    "MCT will quantize the model and start the GPTQ process to optimize the model's parameters and quantization parameters.\n",
    "\n",
    "In addition, we need to define a `TargetPlatformCapability` object, representing the HW specifications on which we wish to eventually deploy our quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edacb5b7779e4d8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Create a GPTQ quantization configuration and set the number of training iterations. \n",
    "# 50 epochs are sufficient for this tutorial. For GPTQ run after mixed precision quantization a higher number of iterations\n",
    "# will be required.\n",
    "gptq_config = mct.gptq.get_pytorch_gptq_config(n_epochs=50)\n",
    "\n",
    "# Specify the target platform capability (TPC)\n",
    "tpc = mct.get_target_platform_capabilities(\"pytorch\", 'imx500', target_platform_version='v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162dd6dd1fce7ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Run model Gradient-based Post-Training Quantization\n",
    "Finally, we quantize our model using MCT's GPTQ API (this may take several minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
   "metadata": {
    "id": "33f8373a-82a5-4b97-9a10-25ee2341d148"
   },
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.gptq.pytorch_gradient_post_training_quantization(\n",
    "    float_model,\n",
    "    representative_dataset_gen,\n",
    "    gptq_config=gptq_config,\n",
    "    target_platform_capabilities=tpc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382ada6-d001-4564-907d-767fa4e9ec56",
   "metadata": {
    "id": "7382ada6-d001-4564-907d-767fa4e9ec56"
   },
   "source": [
    "That's it! Our model is now quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b",
   "metadata": {
    "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b"
   },
   "source": [
    "## Models evaluation\n",
    "\n",
    "In order to evaluate our models, we first need to load the validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c875-c4fc-4819-97e5-721805cba546",
   "metadata": {
    "id": "eef7c875-c4fc-4819-97e5-721805cba546",
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889d217-90a6-4615-8569-38dc9cdd5999",
   "metadata": {
    "id": "9889d217-90a6-4615-8569-38dc9cdd5999"
   },
   "source": [
    "Now, we will create a function for evaluating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209",
   "metadata": {
    "id": "1d3a0ae9-beaa-4af8-8481-49d4917c2209"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model using a test loader.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_acc = (100 * correct / total)\n",
    "    print('Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c",
   "metadata": {
    "id": "ead4a6f3-86a0-4e6c-8229-a2ff514f7b8c"
   },
   "source": [
    "Let's start with the floating-point model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd80fd55ac5b3db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(float_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbd17fa074f472",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, let's evaluate the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848",
   "metadata": {
    "id": "1bc377ee-39b4-4ced-95db-f7d51ab60848"
   },
   "outputs": [],
   "source": [
    "evaluate(quantized_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbb4de-5b6e-4732-83d3-a21e96cdd866",
   "metadata": {
    "id": "ebfbb4de-5b6e-4732-83d3-a21e96cdd866"
   },
   "source": [
    "You can see that we got a very small degradation with a compression rate of x4 !\n",
    "Now, we can export the model to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z3CA16-ojoFL",
   "metadata": {
    "id": "z3CA16-ojoFL"
   },
   "outputs": [],
   "source": [
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path='qmodel.onnx', repr_dataset=representative_dataset_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCT with gradient-based optimization with a few lines of code. We saw that we can achieve an x4 compression ratio with minimal performance degradation.\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2023 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
