{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Export a Quantized Pytorch Model With the Model Compression Toolkit (MCT)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_export.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to export a PyTorch model to ONNX and TorchSript formats using the Model Compression Toolkit (MCT). It covers the steps of creating a simple PyTorch model, applying post-training quantization (PTQ) using MCT, and then exporting the quantized model to ONNX and TorchSript. The tutorial also shows how to use the exported model for inference.\n",
    "\n",
    "## Summary:\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Constructing a simple PyTorch model for demonstration purposes.\n",
    "2. Applying post-training quantization to the model using the Model Compression Toolkit.\n",
    "3. Exporting the quantized model to the ONNX and TorchScript formats.\n",
    "4. Ensuring compatibility between PyTorch and ONNX during the export process.\n",
    "5. Using the exported model for inference.\n",
    "\n",
    "## Setup\n",
    "To export your quantized model to ONNX format and use it for inference, you will need to install some additional packages. Note that these packages are only required if you plan to export the model to ONNX. If ONNX export is not needed, you can skip this step."
   ],
   "metadata": {
    "id": "UJDzewEYfSN5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install -q onnx onnxruntime \"onnxruntime-extensions<0.14\""
   ],
   "metadata": {
    "id": "qNddNV6TEsX0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install the Model Compression Toolkit:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "id": "g10bFms8jzln"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.models.mobilenetv2 import mobilenet_v2\n",
    "import model_compression_toolkit as mct"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantize the Model with the Model Compression Toolkit (MCT)\n",
    "Let's begin the export demonstration by loading a model and applying quantization using MCT. This process will allow us to prepare the model for ONNX export."
   ],
   "metadata": {
    "id": "Q36T6YpZkeTC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a model\n",
    "float_model = mobilenet_v2()\n",
    "\n",
    "# Notice that here the representative dataset is random for demonstration only.\n",
    "def representative_data_gen():\n",
    "    yield [np.random.random((1, 3, 224, 224))]\n",
    "\n",
    "\n",
    "quantized_exportable_model, _ = mct.ptq.pytorch_post_training_quantization(float_model, representative_data_gen=representative_data_gen)\n"
   ],
   "metadata": {
    "id": "eheBYKxRDFgx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "### ONNX\n",
    "The model will be exported in ONNX format, where both weights and activations are represented as floats. Make sure that `onnx` is installed to enable exporting.\n",
    "\n",
    "There are two optional formats available for export: MCTQ or FAKELY_QUANT.\n",
    "\n",
    "#### MCTQ Quantization Format\n",
    "By default, `mct.exporter.pytorch_export_model`  exports the quantized PyTorch model to ONNX using custom quantizers from the `mct_quantizers` module. "
   ],
   "metadata": {
    "id": "-n70LVe6DQPw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path of exported model\n",
    "onnx_file_path = 'model_format_onnx_mctq.onnx'\n",
    "\n",
    "# Export ONNX model with mctq quantizers.\n",
    "mct.exporter.pytorch_export_model(\n",
    "    model=quantized_exportable_model,\n",
    "    save_model_path=onnx_file_path,\n",
    "    repr_dataset=representative_data_gen)"
   ],
   "metadata": {
    "id": "PO-Hh0bzD1VJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the model's size remains unchanged compared to the quantized exportable model, as the weight data types are still represented as floats.\n",
    "\n",
    "#### ONNX Opset Version\n",
    "By default, the ONNX opset version used is 15. However, this can be adjusted by specifying the `onnx_opset_version` parameter during export."
   ],
   "metadata": {
    "id": "Bwx5rxXDF_gb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Export ONNX model with mctq quantizers.\n",
    "mct.exporter.pytorch_export_model(\n",
    "    model=quantized_exportable_model,\n",
    "    save_model_path=onnx_file_path,\n",
    "    repr_dataset=representative_data_gen,\n",
    "    onnx_opset_version=16)"
   ],
   "metadata": {
    "id": "S9XtcX8s3dU9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the Exported Model for Inference\n",
    "To load and perform inference with the ONNX model exported in MCTQ format, use the `mct_quantizers` method `get_ort_session_options` during the creation of an ONNX Runtime session. \n",
    "**Note:** Inference on models exported in this format tends to be slower and experiences higher latency. However, inference on hardware such as the IMX500 will not suffer from this issue."
   ],
   "metadata": {
    "id": "OygCt_iHQQiz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import mct_quantizers as mctq\n",
    "import onnxruntime as ort\n",
    "\n",
    "sess = ort.InferenceSession(onnx_file_path,\n",
    "                            mctq.get_ort_session_options(),\n",
    "                            providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "_input_data = next(representative_data_gen())[0].astype(np.float32)\n",
    "_model_output_name = sess.get_outputs()[0].name\n",
    "_model_input_name = sess.get_inputs()[0].name\n",
    "\n",
    "# Run inference\n",
    "predictions = sess.run([_model_output_name], {_model_input_name: _input_data})"
   ],
   "metadata": {
    "id": "VJL7KkLjRImb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fakely-Quantized Format\n",
    "To export a fakely-quantized model, use the `QuantizationFormat.FAKELY_QUANT` option. This format ensures that quantization is simulated but does not alter the data types of the weights and activations during export."
   ],
   "metadata": {
    "id": "Uf4SbpNC28GA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "\n",
    "# Path of exported model\n",
    "_, onnx_file_path = tempfile.mkstemp('.onnx')\n",
    "\n",
    "# Use QuantizationFormat.FAKELY_QUANT for fakely-quantized weights and activations.\n",
    "mct.exporter.pytorch_export_model(model=quantized_exportable_model,\n",
    "                                  save_model_path=onnx_file_path,\n",
    "                                  repr_dataset=representative_data_gen,\n",
    "                                  quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
   ],
   "metadata": {
    "id": "WLyHEEiwGByT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the fakely-quantized model has the same size as the quantized exportable model, as the weights are still represented as floats.\n",
    "\n",
    "### TorchScript Format\n",
    "\n",
    "The model can also be exported in TorchScript format, where weights and activations are quantized but represented as floats (fakely quantized)."
   ],
   "metadata": {
    "id": "-L1aRxFGGFeF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path of exported model\n",
    "_, torchscript_file_path = tempfile.mkstemp('.pt')\n",
    "\n",
    "\n",
    "# Use mode PytorchExportSerializationFormat.TORCHSCRIPT a torchscript model\n",
    "# and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights and activations.\n",
    "mct.exporter.pytorch_export_model(model=quantized_exportable_model,\n",
    "                                  save_model_path=torchscript_file_path,\n",
    "                                  repr_dataset=representative_data_gen,\n",
    "                                  serialization_format=mct.exporter.PytorchExportSerializationFormat.TORCHSCRIPT,\n",
    "                                  quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
   ],
   "metadata": {
    "id": "V4I-p1q5GLzs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the fakely-quantized model retains the same size as the quantized exportable model, as the weight data types remain in float format."
   ],
   "metadata": {
    "id": "SBqtJV9AGRzN"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "## Copyrights:\n",
    "Copyright 2024 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ]
}
