{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Structured Pruning of a Fully-Connected PyTorch Model using the Model Compression Toolkit (MCT)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_pruning_mnist.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial provides a step-by-step guide to training, pruning, and finetuning a PyTorch fully connected neural network model using the Model Compression Toolkit (MCT). We will start by building and training the model from scratch on the MNIST dataset, followed by applying structured pruning to reduce the model size.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. **Training a PyTorch model on MNIST:** We'll begin by constructing a basic fully connected neural network and training it on the MNIST dataset. \n",
    "2. **Applying structured pruning:** We'll introduce a pruning technique to reduce model size while maintaining performance. \n",
    "3. **Finetuning the pruned model:** After pruning, we'll finetune the model to recover any lost accuracy. \n",
    "4. **Evaluating the pruned model:** We'll evaluate the pruned modelâ€™s performance and compare it to the original model.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f2fe8612d323dd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45e5057240e9db2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac1bac87df87eb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eea35a06ae612b5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a Pytorch classifier model on MNIST\n",
    "Next, we'll define a function to train our neural network model. This function will handle the training loop, including forward propagation, loss calculation, backpropagation, and updating the model parameters. Additionally, we'll evaluate the model's performance on the validation dataset at the end of each epoch to monitor its accuracy. The following code snippets are adapted from the official [PyTorch examples](https://github.com/pytorch/examples/blob/main/mnist/main.py)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e159019685961bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "    return accuracy \n",
    "\n",
    "random_seed = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc1cd6067ea0edb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Fully-Connected Model\n",
    "In this section, we create a simple example of a fully connected model to demonstrate the pruning process. It consists of three linear layers with 128, 64, and 10 neurons."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e035b343d63af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the Fully-Connected Model\n",
    "class FCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.fc_layers(x)\n",
    "        output = F.log_softmax(logits, dim=1)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b77ae732359978b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Preprocessing MNIST Dataset\n",
    "Let's define the dataset loaders to retrieve the train and test parts of the MNIST dataset, including preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "567482fb76082cfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "test_batch_size = 1000\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset_folder = './mnist'\n",
    "train_dataset = datasets.MNIST(dataset_folder, train=True, download=True,\n",
    "                   transform=transform)\n",
    "test_dataset = datasets.MNIST(dataset_folder, train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, pin_memory=True, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, pin_memory=True,  batch_size=test_batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ba3424b2ac17a66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Dense Model\n",
    "We will now train the dense model using the MNIST dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "219e047aa790e812"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "lr = 0.001\n",
    "\n",
    "dense_model = FCModel().to(device)\n",
    "optimizer = optim.Adam(dense_model.parameters(), lr=lr)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(dense_model, device, train_loader, optimizer, epoch)\n",
    "    test(dense_model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37ef306565024207"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dense Model Properties\n",
    "We will display our model's architecture, including layers, their types, and the number of parameters.\n",
    "Notably, MCT's structured pruning will target the first two dense layers for pruning, as these layers  have a higher number of channels compared to later layers, offering more opportunities for pruning without affecting accuracy significantly. This reduction can be effectively propagated by adjusting the input channels of subsequent layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1566c1c7bbc7cf79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_model_params(model):\n",
    "    model_params = sum(p.numel() for p in model.parameters())\n",
    "    for name, module in model.named_modules():\n",
    "        module_params = sum(p.numel() for p in module.state_dict().values())\n",
    "        if module_params > 0:\n",
    "            print(f'{name} number of parameters {module_params}')\n",
    "    print(f'\\nTotal number of parameters {model_params}')\n",
    "    return model_params\n",
    "\n",
    "dense_model_params = display_model_params(dense_model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e661b6cb0414e90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create a Representative Dataset\n",
    "We are creating a representative dataset to guide the pruning process for computing importance score for each channel:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7297a549c27a4b8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter=10\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(train_loader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f22aab1989c92e25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Pruning\n",
    "We are now ready to perform the actual pruning using MCTâ€™s `pytorch_pruning_experimental` function. The model will be pruned based on the defined resource utilization constraints and the previously generated representative dataset.\n",
    "\n",
    "Each channelâ€™s importance is measured using the [LFH (Label-Free-Hessian) method](https://arxiv.org/abs/2309.11531), which approximates the Hessian of the loss function with respect to the modelâ€™s weights.\n",
    "\n",
    "For efficiency, we use a single score approximation. Although less precise, it significantly reduces processing time compared to multiple approximations, which offer better accuracy but at the cost of longer runtimes.\n",
    "\n",
    "MCTâ€™s structured pruning will target the first two dense layers, where output channel reduction can be propagated to subsequent layers by adjusting their input channels accordingly.\n",
    "\n",
    "The output is a pruned model along with pruning information, including layer-specific pruning masks and scores."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ae781eb7d420ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "compression_ratio = 0.5\n",
    "\n",
    "# Define Resource Utilization constraint for pruning. Each float32 parameter requires 4 bytes, hence we multiply the total parameter count by 4 to calculate the memory footprint.\n",
    "target_resource_utilization = mct.core.ResourceUtilization(weights_memory=dense_model_params * 4 * compression_ratio)\n",
    "\n",
    "# Define a pruning configuration\n",
    "pruning_config=mct.pruning.PruningConfig(num_score_approximations=1)\n",
    "\n",
    "# Prune the model\n",
    "pruned_model, pruning_info = mct.pruning.pytorch_pruning_experimental(\n",
    "    model=dense_model,\n",
    "    target_resource_utilization=target_resource_utilization, \n",
    "    representative_data_gen=representative_dataset_gen, \n",
    "    pruning_config=pruning_config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96f9ca0490343c18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model after pruning\n",
    "Let us view the model after the pruning operation and check the accuracy. We can see that pruning process caused a degradation in accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad14328ce33ecb97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pruned_model_nparams = display_model_params(pruned_model)\n",
    "acc_before_finetuning = test(pruned_model, device, test_loader)\n",
    "print(f'Pruned model accuracy before finetuning {acc_before_finetuning}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85ee17d3804a61bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetuning the Pruned Model\n",
    "After pruning, we often need to finetune the model to recover any lost performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fd438bff45aded3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(pruned_model.parameters(), lr=lr)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(pruned_model, device, train_loader, optimizer, epoch)\n",
    "    test(pruned_model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81250c2caca111a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can export the quantized model to ONNX:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29c044b7180c42c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mct.exporter.pytorch_export_model(pruned_model, save_model_path='qmodel.onnx', repr_dataset=representative_dataset_gen)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be1eec6652169d4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusions\n",
    "In this tutorial, we demonstrated the process of training, pruning, and finetuning a neural network model using MCT. We began by setting up our environment and loading the dataset, followed by building and training a fully connected neural network. We then introduced the concept of model pruning, specifically targeting the first two dense layers to efficiently reduce the model's memory footprint by 50%. After applying structured pruning, we evaluated the pruned model's performance and concluded the tutorial by fine-tuning the pruned model to recover any lost accuracy due to the pruning process. This tutorial provided a hands-on approach to model optimization through pruning, showcasing the balance between model size, performance, and efficiency.\n",
    "\n",
    "## Copyrights\n",
    "Copyright 2024 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a927746e0f8d2f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
