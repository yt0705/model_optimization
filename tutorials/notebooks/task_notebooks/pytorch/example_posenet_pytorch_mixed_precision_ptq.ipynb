{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb4870d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# PoseNet and Mixed-Precision Post-Training Quantization in PyTorch using the Model Compression Toolkit(MCT)\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a PoseNet model. We will load a pre-trained model and quantize it using the MCT with **Mixed-Precision Post-Training Quantization (PTQ)** .\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing COCO’s dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## posenet-pytorch(Dependent External Repository)\n",
    "This tutorial uses the repository linked below. Installation instructions are provided in the **Setup** section.  \n",
    "This repository accesses Google's TensorFlow.js version of the PoseNet model and converts the retrieved model into a PyTorch model.  \n",
    "The model uses MobileNetV1 as its backbone.  \n",
    "You can choose from four model depths: 50, 75, 100, and 101.Model selection can be configured in the **Parameter setting** section described later.  \n",
    "[posenet-pytorch](https://github.com/michellelychan/posenet-pytorch)\n",
    "\n",
    "### License(posenet-pytorch)\n",
    "Copyright 2018 Ross Wightman\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "## Setup \n",
    "First, clone the GitHub repository.\n",
    "This repository is mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a087b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('posenet-pytorch'):\n",
    "    !git clone https://github.com/michellelychan/posenet-pytorch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bc6ac",
   "metadata": {},
   "source": [
    "In the `__init__.py` file within the cloned repository (posenet-pytorch/posenet/\\_\\_init\\_\\_.py), the function `decode_multiple_poses` is currently disabled by being commented out. Therefore, enable it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3204bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i '2s/^# *\\(from .* import .*\\)/\\1/' ./posenet-pytorch/posenet/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64e375",
   "metadata": {},
   "source": [
    "```python\n",
    "# ./posenet-pytorch/posenet/__init__.py\n",
    "from posenet.constants import *\n",
    "from posenet.decode_multi import decode_multiple_poses  # <-- this sentence\n",
    "from posenet import decode\n",
    "from posenet.models.model_factory import load_model\n",
    "from posenet.models import MobileNetV1, MOBILENET_V1_CHECKPOINTS\n",
    "from posenet.utils import *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df95c",
   "metadata": {},
   "source": [
    "Install the relevant packages:  \n",
    "This step may take several minutes...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0\n",
    "!pip install onnx==1.16.1\n",
    "!pip install numpy==1.26.4\n",
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install pycocotools==2.0.10\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "57c3f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0dba768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./posenet-pytorch')\n",
    "sys.path.append('./posenet-pytorch/posenet')\n",
    "import posenet\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766698",
   "metadata": {},
   "source": [
    "### Various Settings\n",
    "Here, you can configure the parameters listed below.  \n",
    "\n",
    "#### File path setting\n",
    "- SAVE_FLOAT_EVAL_RESULT  \n",
    "  This parameter sets the filename for outputting inference results before quantization.\n",
    "- SAVE_QUANT_EVAL_RESULT  \n",
    "  This parameter sets the filename for outputting inference results after quantization.\n",
    "\n",
    "#### Parameter setting\n",
    "- MODEL_ID  \n",
    "  This parameter allows you to select the model depth to use(50, 75, 100, 101).\n",
    "- SCALE_FACTOR  \n",
    "  This parameter allows you to set the scaling for the input image.\n",
    "- DECODE_MAX_POSES  \n",
    "  This parameter allows you to set the maximum number of detections in pose estimation.\n",
    "- DECODE_MIN_POSE_SCORE  \n",
    "  This parameter allows you to set the minimum score for pose detection.\n",
    "- KPT_VIS_THR  \n",
    "  This parameter allows you to set the visibility of keypoints.\n",
    "- NUM_WORKERS  \n",
    "  This parameter allows you to set the number of processes for parallelizing the data loading process.\n",
    "- CALIB_ITER  \n",
    "  This parameter allows you to set how many samples to use when generating representative data for quantization.\n",
    "- WEIGHTS_COMPRESSION_RATIO  \n",
    "  This parameter allows you to set the quantization ratio based on the weight size of the 8-bit model when using mixed-precision quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path setting\n",
    "SAVE_FLOAT_EVAL_RESULT = \"float_eval_result\"\n",
    "SAVE_QUANT_EVAL_RESULT = \"quant_eval_result\"\n",
    "\n",
    "# Parameter setting\n",
    "MODEL_ID = 75\n",
    "SCALE_FACTOR = 1.0\n",
    "DECODE_MAX_POSES = 20\n",
    "DECODE_MIN_POSE_SCORE = 0\n",
    "KPT_LAB_THR = 0.2\n",
    "KPT_VIS_THR = 0.5\n",
    "NUM_WORKERS = 0\n",
    "CALIB_ITER = 10\n",
    "IMG_HEIGHT = 480\n",
    "IMG_WIDTH = 640\n",
    "WEIGHTS_COMPRESSION_RATIO = 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928c12f",
   "metadata": {},
   "source": [
    "Load a pre-trained PoseNet(MobileNetV1 backbone) model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_model = posenet.load_model(MODEL_ID)\n",
    "output_stride = getattr(float_model, 'output_stride', 8)\n",
    "print(output_stride)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e43f6",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "When you run the code for the first time, the model download will begin.  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d437",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "### Download COCO's dataset\n",
    "\n",
    "**Note**  \n",
    "In this tutorial, we will use a subset of COCO train2017 for calibration during quantization and COCO val2017 for evaluation.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "afe82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('COCO_dataset'):\n",
    "    !mkdir COCO_dataset\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/zips/train2017.zip\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/zips/val2017.zip\n",
    "    !unzip COCO_dataset/annotations_trainval2017.zip -d COCO_dataset\n",
    "    !unzip COCO_dataset/train2017.zip -d COCO_dataset\n",
    "    !unzip COCO_dataset/val2017.zip -d COCO_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe934651",
   "metadata": {},
   "source": [
    "Here, we are setting the paths for the annotation file and image folder of the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "36d8913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_TRAIN_IMG_DIR = \"COCO_dataset/train2017/\"\n",
    "COCO_VAL_IMG_DIR = \"COCO_dataset/val2017/\"\n",
    "COCO_TRAIN_ANN_JSON = \"COCO_dataset/annotations/person_keypoints_train2017.json\"\n",
    "COCO_VAL_ANN_JSON = \"COCO_dataset/annotations/person_keypoints_val2017.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c60b0",
   "metadata": {},
   "source": [
    "In this class, we process the downloaded COCO's dataset for calibration during quantization and for use in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoPoseNetDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, ann_json: str, output_stride: int, scale_factor: float = 1.0):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_json)\n",
    "        self.img_ids = self.coco.getImgIds(catIds=[1])\n",
    "        self.output_stride = output_stride\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "\n",
    "        input_image, draw_image, output_scale = posenet.read_imgfile(\n",
    "            img_path, scale_factor=self.scale_factor, output_stride=self.output_stride\n",
    "        )\n",
    "\n",
    "        input_image = np.squeeze(input_image, axis=0)\n",
    "        input_image = input_image.transpose((1, 2, 0))\n",
    "        input_image = cv2.resize(input_image,(IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_LINEAR)\n",
    "        input_image = input_image.transpose((2, 0, 1)).reshape(1, 3, IMG_HEIGHT, IMG_WIDTH)\n",
    "        input_image = input_image.astype(np.float32)\n",
    "\n",
    "        output_scale_height, output_scale_width, _ = draw_image.shape\n",
    "        output_scale = np.array([output_scale_height / IMG_HEIGHT, output_scale_width / IMG_WIDTH])\n",
    "            \n",
    "        input_tensor = torch.from_numpy(input_image)\n",
    "\n",
    "        sample = {\n",
    "            'input': input_tensor,\n",
    "            'img_id': img_id,\n",
    "            'output_scale': output_scale,\n",
    "            'file_name': img_info['file_name'],\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418a6f6",
   "metadata": {},
   "source": [
    "Generate an array of keypoints with visibility/invisibility flags based on keypoint information and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "441e5aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_kpts_xy_score_to_xyv(\n",
    "    kpts_xy: np.ndarray, kpts_score: np.ndarray,\n",
    "    visible_thr: float = 0.5, label_thr: float = 0.2) -> np.ndarray:\n",
    "\n",
    "    # COCO v Format Extension: Assign 0/1/2 based on score\n",
    "    # visible_thr: Threshold for determining visibility\n",
    "    # label_thr: Threshold for determining presence of label\n",
    "    \n",
    "    # v=0: does not exist within the image (< label_thr)\n",
    "    vis = np.zeros_like(kpts_score, dtype=np.int32)\n",
    "    # v=2: Completely visible (> visible_thr)\n",
    "    vis[kpts_score > visible_thr] = 2\n",
    "    # v=1: The label is present but not visible within the image (> label_thr, < visible_thr)\n",
    "    vis[(kpts_score > label_thr) & (kpts_score <= visible_thr)] = 1\n",
    "    # Combination to [x, y, v]\n",
    "    kpts_xyv = np.concatenate([kpts_xy, vis[:, None]], axis=1)\n",
    "    return kpts_xyv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf93f43",
   "metadata": {},
   "source": [
    "Organize key point information into a one-dimensional list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ce857517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_xyv(kpts_xyv: np.ndarray) -> List[float]:\n",
    "    return [float(v) for row in kpts_xyv for v in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CocoPoseNetDataset(\n",
    "    img_dir=COCO_VAL_IMG_DIR, ann_json=COCO_VAL_ANN_JSON,\n",
    "    output_stride=output_stride, scale_factor=SCALE_FACTOR\n",
    ")\n",
    "\n",
    "calib_dataset = CocoPoseNetDataset(\n",
    "    img_dir=COCO_TRAIN_IMG_DIR, ann_json=COCO_TRAIN_ANN_JSON,\n",
    "    output_stride=output_stride, scale_factor=SCALE_FACTOR\n",
    ")\n",
    "\n",
    "# For evaluation (batch size 1)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=lambda x: x[0]\n",
    ")\n",
    "\n",
    "# For calibration（No label required）\n",
    "calib_loader = DataLoader(\n",
    "    calib_dataset, batch_size=1, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=lambda x: x[0]\n",
    ")\n",
    "\n",
    "print(len(calib_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2915a6",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7bba02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen():\n",
    "    for sample in itertools.islice(itertools.cycle(calib_loader), CALIB_ITER):\n",
    "        yield [sample['input']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea063a",
   "metadata": {},
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/modules/target_platform_capabilities.html)). Here, we use the default Pytorch TPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5d5f5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "tpc = mct.get_target_platform_capabilities('pytorch', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d232bb",
   "metadata": {},
   "source": [
    "## Mixed Precision Configurations\n",
    "We will create a `MixedPrecisionQuantizationConfig` that defines the search options for mixed-precision:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25783c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = mct.core.CoreConfig(\n",
    "    mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=CALIB_ITER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Resource Utilization information to constraint your model's memory size.\n",
    "resource_utilization_data = mct.core.pytorch_resource_utilization_data(\n",
    "    float_model,\n",
    "    representative_dataset_gen,\n",
    "    configuration,\n",
    "    target_platform_capabilities=tpc)\n",
    "\n",
    "# Create a ResourceUtilization object \n",
    "resource_utilization = mct.core.ResourceUtilization(resource_utilization_data.weights_memory * WEIGHTS_COMPRESSION_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fcf4d",
   "metadata": {},
   "source": [
    "# Post-Training Quantization using MCT\n",
    "Now for the exciting part! Let's run PTQ on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "                                        in_module=float_model,\n",
    "                                        representative_data_gen=representative_dataset_gen,\n",
    "                                        target_platform_capabilities=tpc,\n",
    "                                        core_config=configuration,\n",
    "                                        target_resource_utilization=resource_utilization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b178",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Now, we will create a function for evaluating a model.  \n",
    "The inference results before and after quantization are displayed on the terminal and simultaneously written to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: torch.nn.Module,\n",
    "             val_dataset: CocoPoseNetDataset,\n",
    "             val_dataloader: DataLoader,\n",
    "             decode_max_poses: int = 1,\n",
    "             decode_min_pose_score: float = 0,\n",
    "             kpt_lab_thr: float = 0.2,\n",
    "             kpt_vis_thr: float = 0.5) -> float:\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    output_stride = val_dataset.output_stride\n",
    "\n",
    "    results = []\n",
    "    for sample in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        inp = sample['input'].to(device)\n",
    "        img_id = sample['img_id']\n",
    "        output_scale = sample['output_scale']\n",
    "\n",
    "        heat, off, disp_f, disp_b = model(inp)\n",
    "        heat, off, disp_f, disp_b = heat.squeeze(0), off.squeeze(0), disp_f.squeeze(0), disp_b.squeeze(0)\n",
    "\n",
    "        # decode\n",
    "        pose_scores, keypoint_scores, keypoint_coords, pose_offsets = posenet.decode_multiple_poses(\n",
    "            heat,\n",
    "            off,\n",
    "            disp_f,\n",
    "            disp_b,\n",
    "            output_stride=output_stride,\n",
    "            max_pose_detections=decode_max_poses,\n",
    "            min_pose_score=decode_min_pose_score)\n",
    "\n",
    "        for p_idx, ps in enumerate(pose_scores):\n",
    "            if ps == 0.0:\n",
    "                continue\n",
    "            kpts_xy = keypoint_coords[p_idx]\n",
    "            kpts_xy_img = np.zeros_like(kpts_xy)\n",
    "            kpts_xy_img[:, 0] = kpts_xy[:, 1]* output_scale[1]\n",
    "            kpts_xy_img[:, 1] = kpts_xy[:, 0]* output_scale[0]\n",
    "            kpts_sc = keypoint_scores[p_idx]\n",
    "            kpts_xyv = coco_kpts_xy_score_to_xyv(kpts_xy_img, kpts_sc, visible_thr=kpt_vis_thr, label_thr=kpt_lab_thr)\n",
    "            keypoint = flatten_xyv(kpts_xyv)\n",
    "            results.append({\n",
    "                \"image_id\": int(img_id),\n",
    "                \"category_id\": 1,\n",
    "                \"keypoints\": keypoint,\n",
    "                \"score\": float(ps)\n",
    "            })\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"WARNING : No detection results found. Returning AP=0.0.\")\n",
    "        return\n",
    "    \n",
    "    if model==float_model:\n",
    "        with open(os.path.join(SAVE_FLOAT_EVAL_RESULT + '.json'), 'w') as f:\n",
    "            json.dump(results,f,ensure_ascii=False,indent=1)\n",
    "    else:\n",
    "        with open(os.path.join(SAVE_QUANT_EVAL_RESULT + '.json'), 'w') as f:\n",
    "            json.dump(results,f,ensure_ascii=False,indent=1)\n",
    "\n",
    "    # evaluation\n",
    "    coco_gt = val_dataset.coco\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    evaluator = COCOeval(coco_gt, coco_dt, iouType='keypoints')\n",
    "    evaluator.evaluate()\n",
    "    evaluator.accumulate()\n",
    "    evaluator.summarize()\n",
    "    ap = float(evaluator.stats[0])\n",
    "    print(f\"AP (OKS mAP): {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eb053",
   "metadata": {},
   "source": [
    "Let's start with the floating-point model evaluation.  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating float model（COCO mAP）...\")\n",
    "evaluate(float_model,\n",
    "            val_dataset,\n",
    "            val_dataloader,\n",
    "            decode_max_poses=DECODE_MAX_POSES,\n",
    "            decode_min_pose_score=DECODE_MIN_POSE_SCORE,\n",
    "            kpt_vis_thr=KPT_VIS_THR,\n",
    "            kpt_lab_thr=KPT_LAB_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef5d0b",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the quantized model:  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba05924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating quantized model（COCO mAP）...\")\n",
    "evaluate(quantized_model,\n",
    "            val_dataset,\n",
    "            val_dataloader,\n",
    "            decode_max_poses=DECODE_MAX_POSES,\n",
    "            decode_min_pose_score=DECODE_MIN_POSE_SCORE,\n",
    "            kpt_vis_thr=KPT_VIS_THR,\n",
    "            kpt_lab_thr=KPT_LAB_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f29a",
   "metadata": {},
   "source": [
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-mct242 (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
