{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb4870d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Deeplabv3+ and Mixed-Precision Post-Training Quantization in PyTorch using the Model Compression Toolkit(MCT)\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a DeepLabv3+ semantic segmentation model. We will load a pre-trained model and quantize it using the MCT with **Mixed-Precision Post-Training Quantization (PTQ)** .\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing PASCAL VOC's dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## DeepLabV3Plus-Pytorch(Dependent External Repository)\n",
    "This tutorial uses the repository linked below. Installation instructions are provided in the **Setup** section.  \n",
    "The model uses MobileNetV2 as its backbone.\n",
    "[DeepLabV3Plus-Pytorch](https://github.com/VainF/DeepLabV3Plus-Pytorch)\n",
    "\n",
    "### License(DeepLabV3Plus-Pytorch)\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2020 Gongfan Fang\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "## Setup \n",
    "First, clone the GitHub repository.\n",
    "This repository is mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('DeepLabV3Plus-Pytorch'):\n",
    "    !git clone https://github.com/VainF/DeepLabV3Plus-Pytorch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312cf1b",
   "metadata": {},
   "source": [
    "Next, please download the pre-trained model **best_deeplabv3plus_mobilenet_voc_os16.pth** from the following link. (Dropbox or Tencent Weiyun)  \n",
    "[1.Available Architectures(DeepLabV3Plus-Pytorch)](https://github.com/VainF/DeepLabV3Plus-Pytorch?tab=readme-ov-file#1-available-architectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244bc6ac",
   "metadata": {},
   "source": [
    "We will modify **DeepLabV3Plus-Pytorch/network/modeling.py** to set dilation to only 1 for IMX500-compatible models.  \n",
    "This modification may slightly reduce accuracy.  \n",
    "Run the following command to apply the modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204bbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/    aspp_dilate = \\[12, 24, 36\\]/    aspp_dilate = [1, 1, 1] #aspp_dilate = [12, 24, 36]/g' ./DeepLabV3Plus-Pytorch/network/modeling.py\n",
    "!sed -i 's/    aspp_dilate = \\[6, 12, 18\\]/    aspp_dilate = [1, 1, 1] #aspp_dilate = [6, 12, 18]/g' ./DeepLabV3Plus-Pytorch/network/modeling.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64e375",
   "metadata": {},
   "source": [
    "```python\n",
    "def _segm_mobilenet(name, backbone_name, num_classes, output_stride, pretrained_backbone):\n",
    "    if output_stride==8:\n",
    "        aspp_dilate = [1, 1, 1] #aspp_dilate = [12, 24, 36]\n",
    "    else:\n",
    "        aspp_dilate = [1, 1, 1] #aspp_dilate = [6, 12, 18]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df95c",
   "metadata": {},
   "source": [
    "Install the relevant packages:  \n",
    "This step may take several minutes...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0\n",
    "!pip install onnx==1.16.1\n",
    "!pip install numpy==1.26.4\n",
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import transforms\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from torchmetrics import JaccardIndex\n",
    "sys.path.append('./DeepLabV3Plus-Pytorch')\n",
    "import network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766698",
   "metadata": {},
   "source": [
    "### Various Settings\n",
    "Here, you can configure the parameters listed below.  \n",
    "\n",
    "#### Parameter setting\n",
    "- IMG_HEIGHT, IMG_WIDTH  \n",
    "  This parameter allows you to set the size of input images.\n",
    "- NUM_WORKERS  \n",
    "  This parameter allows you to set the number of processes for parallelizing the data loading process.\n",
    "- CALIB_ITER  \n",
    "  This parameter allows you to set how many samples to use when generating representative data for quantization.\n",
    "- WEIGHTS_COMPRESSION_RATIO  \n",
    "  This parameter allows you to set the quantization ratio based on the weight size of the 8-bit model when using Mixed-precision quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CALIB_ITER = 10\n",
    "NUM_WORKERS = 1\n",
    "WEIGHTS_COMPRESSION_RATIO = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928c12f",
   "metadata": {},
   "source": [
    "Load a pre-trained Deeplabv3+(MobileNetV2 backbone) model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = network.modeling.__dict__[\"deeplabv3plus_mobilenet\"]\n",
    "float_model = model_builder(output_stride=16)\n",
    "float_model.load_state_dict(torch.load( \"best_deeplabv3plus_mobilenet_voc_os16.pth\", weights_only=False)['model_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d437",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "### Download PASCAL VOC's dataset\n",
    "\n",
    "**Note**  \n",
    "In this tutorial, we will use a subset of PASCAL VOC 2012 dataset for calibration during quantization and for evaluation.\n",
    "\n",
    "This step may take several minutes...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('VOC_dataset'):\n",
    "    !mkdir VOC_dataset\n",
    "    !wget -P VOC_dataset https://datasets.cms.waikato.ac.nz/ufdl/data/pascalvoc2012/VOCtrainval_11-May-2012.tar\n",
    "    !tar -xf VOC_dataset/VOCtrainval_11-May-2012.tar -C ./VOC_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99cce1",
   "metadata": {},
   "source": [
    "### Prepare PASCAL VOC's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00da361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess of VOC dataset.\n",
    "transform_img = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_WIDTH,IMG_HEIGHT)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_target = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_WIDTH,IMG_HEIGHT), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "        transforms.PILToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VOCSegmentation(root=\"./VOC_dataset/\", year='2012', image_set='train', transform = transform_img, target_transform=transform_target)\n",
    "val_dataset = VOCSegmentation(root=\"./VOC_dataset/\", year='2012', image_set='val',  transform = transform_img, target_transform=transform_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation (batch size 1)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    \n",
    ")\n",
    "\n",
    "# For calibration（No label required）\n",
    "calib_loader = DataLoader(\n",
    "    train_dataset, batch_size=1, shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2915a6",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen():\n",
    "    for sample in itertools.islice(itertools.cycle(calib_loader), CALIB_ITER):\n",
    "        yield [sample[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea063a",
   "metadata": {},
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/modules/target_platform_capabilities.html)). Here, we use the default Pytorch TPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "tpc = mct.get_target_platform_capabilities('pytorch', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d232bb",
   "metadata": {},
   "source": [
    "## Mixed-Precision Configurations\n",
    "We will create a `MixedPrecisionQuantizationConfig` that defines the search options for Mixed-Precision:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25783c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = mct.core.CoreConfig(\n",
    "    mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=CALIB_ITER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Resource Utilization information to constraint your model's memory size.\n",
    "resource_utilization_data = mct.core.pytorch_resource_utilization_data(\n",
    "    float_model,\n",
    "    representative_dataset_gen,\n",
    "    configuration,\n",
    "    target_platform_capabilities=tpc)\n",
    "\n",
    "# Create a ResourceUtilization object \n",
    "resource_utilization = mct.core.ResourceUtilization(resource_utilization_data.weights_memory * WEIGHTS_COMPRESSION_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fcf4d",
   "metadata": {},
   "source": [
    "# Post-Training Quantization using MCT\n",
    "Now for the exciting part! Let's run PTQ on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "                                        in_module=float_model,\n",
    "                                        representative_data_gen=representative_dataset_gen,\n",
    "                                        target_platform_capabilities=tpc,\n",
    "                                        core_config=configuration,\n",
    "                                        target_resource_utilization=resource_utilization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b178",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Now, we will create a function for evaluating a model.  \n",
    "The inference results before and after quantization are displayed on the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: torch.nn.Module, val_dataloader: DataLoader,\n",
    "             num_classes: int = 21):\n",
    "    \"\"\"\n",
    "    Evaluation of the PASCAL VOC dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Evaluation model.\n",
    "        val_dataloader (DataLoader): Evaluation dataset.\n",
    "        num_classes (int): num of classes(defualt:21)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    miou_metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes, ignore_index=255).to(device)\n",
    "    for sample in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        img, target = sample\n",
    "        img = img.to(device)\n",
    "        target = target.squeeze(1).long().to(device)\n",
    "\n",
    "        logits = model(img)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        miou_metric.update(preds, target)\n",
    "\n",
    "    miou = miou_metric.compute()\n",
    "    print(f\"VOC2012 val mIoU: {miou.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eb053",
   "metadata": {},
   "source": [
    "Let's start with the floating-point model evaluation.  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating float model (VOC mIoU)...\")\n",
    "evaluate(model=float_model, val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef5d0b",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the quantized model:  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba05924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating quantized model (VOC mIoU) ...\")\n",
    "evaluate(model=quantized_model, val_dataloader=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f29a",
   "metadata": {},
   "source": [
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-deeplabv3ptest (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
