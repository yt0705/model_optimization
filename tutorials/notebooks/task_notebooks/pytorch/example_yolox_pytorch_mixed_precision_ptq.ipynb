{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb4870d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# YOLOX and Mixed-Precision Post-Training Quantization in PyTorch using the Model Compression Toolkit(MCT)\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a YOLOX model. We will load a pre-trained model and quantize it using the MCT with **Mixed-Precision Post-Training Quantization (PTQ)** .\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing COCO’s dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## YOLOX(Dependent External Repository)\n",
    "This tutorial uses the repository linked below. Installation instructions are provided in the **Setup** section.   \n",
    "[YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n",
    "\n",
    "### License(YOLOX)\n",
    "   Copyright (c) 2021-2022 Megvii Inc. All rights reserved.\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "\n",
    "## Setup  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646df95c",
   "metadata": {},
   "source": [
    "First, install the relevant packages:  \n",
    "This step may take several minutes...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b9ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.6.0 torchvision==0.21.0\n",
    "!pip install onnx==1.16.1\n",
    "!pip install numpy==1.26.4\n",
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install pycocotools==2.0.10\n",
    "!pip install onnx-simplifier~=0.4.10\n",
    "!pip install loguru\n",
    "!pip install tqdm\n",
    "!pip install thop\n",
    "!pip install ninja\n",
    "!pip install tabulate\n",
    "!pip install psutil\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9cd18",
   "metadata": {},
   "source": [
    "Clone the GitHub repository and install.\n",
    "This repository is mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('YOLOX'):\n",
    "    !git clone https://github.com/Megvii-BaseDetection/YOLOX.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bdbd94",
   "metadata": {},
   "source": [
    "Download a pre-trained YOLOX-Tiny model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1274cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('yolox_tiny.pth'):\n",
    "    !wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_tiny.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from typing import Dict, Tuple, Any\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import model_compression_toolkit as mct\n",
    "from edgemdt_cl.pytorch import FasterRCNNBoxDecode, MulticlassNMS\n",
    "sys.path.append('./YOLOX')\n",
    "from yolox.exp import get_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c766698",
   "metadata": {},
   "source": [
    "### Various Settings\n",
    "Here, you can configure the parameters listed below.  \n",
    "\n",
    "#### Parameter setting\n",
    "- IMG_HEIGHT, IMG_WIDTH  \n",
    "  This parameter allows you to set the size of input images.\n",
    "- SCORE_THR  \n",
    "  This parameter allows you to set the threshold of class score for the Non-Maximum Suppression (NMS) and evaluation.\n",
    "- IOU_THR  \n",
    "  This parameter allows you to set the threshold of iou for the Non-Maximum Suppression (NMS).\n",
    "- NUM_WORKERS  \n",
    "  This parameter allows you to set the number of processes for parallelizing the data loading process.\n",
    "- CALIB_ITER  \n",
    "  This parameter allows you to set how many samples to use when generating representative data for quantization.\n",
    "- WEIGHTS_COMPRESSION_RATIO  \n",
    "  This parameter allows you to set the quantization ratio based on the weight size of the 8-bit model when using mixed-precision quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f4914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setting\n",
    "IMG_HEIGHT = 416\n",
    "IMG_WIDTH = 416\n",
    "SCORE_THR = 0.1\n",
    "IOU_THR = 0.65\n",
    "NUM_WORKERS = 0\n",
    "CALIB_ITER = 10\n",
    "WEIGHTS_COMPRESSION_RATIO = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928c12f",
   "metadata": {},
   "source": [
    "Load a pre-trained YOLOX-Tiny model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = get_exp('./YOLOX/exps/default/yolox_tiny.py', None)\n",
    "float_model = exp.get_model()\n",
    "float_model.head.decode_in_inference = False\n",
    "weights = torch.load('./yolox_tiny.pth', map_location=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "float_model.load_state_dict(weights[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436710a6",
   "metadata": {},
   "source": [
    "Next, we add the CustomLayer (edgemdt_cl) as post-processing.  \n",
    "\n",
    "* FasterRCNNBoxDecode: Decodes YOLOX inference results from Anchor format to BoundingBox format.    \n",
    "* MulticlassNMS: Executes the Non-Maximum Suppression to remove overlapping boxes.\n",
    "\n",
    "Note: YOLOX returns the data of (xc, yc, h, w) but FasterRCNNBoxDecode use the data of (yc, xc, h, w), so convert data to (yc, xc, h, w) before decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b7401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOXPostProcess(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapping YoloX with post process functionality: FasterRCNNBoxDecode and MulticlassNMS from edgemdt_cl.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, img_size: Tuple = (416, 416),\n",
    "                 score_threshold: float = 0.1, iou_threshold: float = 0.65,\n",
    "                 max_detections: int = 200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (torch.nn.Module): Model instance.\n",
    "            img_size (tuple): Image size input of the model.\n",
    "            score_threshold (float): Score threshold for non-maximum suppression.\n",
    "            iou_threshold (float): Intersection over union threshold for non-maximum suppression.\n",
    "            max_detections (int): The number of detections to return.\n",
    "        \"\"\"\n",
    "        super(YOLOXPostProcess, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.img_size = img_size\n",
    "        self.strides = [8, 16, 32]  # strides to bed used in anchors.\n",
    "        self.model = model\n",
    "        self.box_decoder = FasterRCNNBoxDecode(anchors=self.create_anchors(),\n",
    "                                               scale_factors=[1,1,1,1],\n",
    "                                               clip_window=[0,0,*img_size])\n",
    "        self.nms = MulticlassNMS(score_threshold, iou_threshold, max_detections)\n",
    "\n",
    "    def create_anchors(self) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Create anchors for box decoding operation.\n",
    "\n",
    "        Returns: \n",
    "            anchors (torch.tensor): tesnor of anchors.\n",
    "        \"\"\"\n",
    "        fmap_grids = []\n",
    "        fmap_strides = []\n",
    "        hsizes = [self.img_size[0] // stride for stride in self.strides]\n",
    "        wsizes = [self.img_size[1] // stride for stride in self.strides]\n",
    "        for hsize, wsize, stride in zip(hsizes, wsizes, self.strides):\n",
    "            yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
    "            grid = torch.stack((xv, yv), 2).view(1, -1, 2)\n",
    "            fmap_grids.append(grid)\n",
    "            shape = grid.shape[:2]\n",
    "            fmap_strides.append(torch.full((*shape, 1), stride))\n",
    "\n",
    "        s = torch.cat(fmap_strides, dim=1).to(self.device)\n",
    "        offsets = s * torch.cat(fmap_grids, dim=1).to(self.device)\n",
    "        xc, yc = offsets[..., 0:1], offsets[..., 1:2]\n",
    "        anchors = torch.concat([(2 * yc - s) / 2, (2 * xc - s) / 2,\n",
    "                                (2 * yc + s) / 2, (2 * xc + s) / 2], dim=-1)\n",
    "        anchors = anchors.squeeze(0)\n",
    "        return anchors\n",
    "\n",
    "    def forward(self, images: torch.tensor) -> Tuple:\n",
    "        \"\"\"\n",
    "        Forward processing.\n",
    "\n",
    "        Args:\n",
    "            images (torch.tensor): Input images.\n",
    "\n",
    "        Returns:\n",
    "            nms_out.boxes: Bounding boxes after NMS processing.\n",
    "            nms_out.scores: Scores after NMS processing.\n",
    "            nms_out.labels: labels after NMS processing.\n",
    "        \"\"\"        \n",
    "        outputs = self.model(images)\n",
    "        boxes = outputs[..., :4]\n",
    "        # Convert from (xc, yc, w, h) to (yc, xc, h, w)\n",
    "        xc, yc, w, h = boxes[..., 0], boxes[..., 1], boxes[..., 2], boxes[..., 3]\n",
    "        boxes = torch.stack([yc, xc, h, w], dim=-1)\n",
    "        # Box decoder\n",
    "        boxes = self.box_decoder(boxes)\n",
    "        scores = outputs[..., 5:] * outputs[..., 4:5]  # classes * scores\n",
    "        # NMS\n",
    "        nms_out = self.nms(boxes, scores)\n",
    "        return nms_out.boxes, nms_out.scores, nms_out.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089502b",
   "metadata": {},
   "source": [
    "Wrapping YoloX with post process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e10a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_float_model = YOLOXPostProcess(model = torch.nn.Sequential(float_model.backbone, float_model.head),\n",
    "                                    img_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "                                    score_threshold = SCORE_THR, iou_threshold = IOU_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d437",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "### Download COCO's dataset\n",
    "\n",
    "**Note**  \n",
    "In this tutorial, we will use a subset of COCO train2017 for calibration during quantization and COCO val2017 for evaluation.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe82c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('COCO_dataset'):\n",
    "    !mkdir COCO_dataset\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/zips/train2017.zip\n",
    "    !wget -P COCO_dataset http://images.cocodataset.org/zips/val2017.zip\n",
    "    !unzip COCO_dataset/annotations_trainval2017.zip -d COCO_dataset\n",
    "    !unzip COCO_dataset/train2017.zip -d COCO_dataset\n",
    "    !unzip COCO_dataset/val2017.zip -d COCO_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe934651",
   "metadata": {},
   "source": [
    "Here, we are setting the paths for the annotation file and image folder of the downloaded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_TRAIN_IMG_DIR = \"COCO_dataset/train2017/\"\n",
    "COCO_VAL_IMG_DIR = \"COCO_dataset/val2017/\"\n",
    "COCO_TRAIN_ANN_JSON = \"COCO_dataset/annotations/instances_train2017.json\"\n",
    "COCO_VAL_ANN_JSON = \"COCO_dataset/annotations/instances_val2017.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c60b0",
   "metadata": {},
   "source": [
    "In this class, we process the downloaded COCO's dataset for calibration during quantization and for use in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Define the COCO dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir: str, ann_json: str, img_size: Tuple = (416,416)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (str): Data folder path.\n",
    "            ann_json (str): Annotation file name.\n",
    "            img_size (tuple): Image size input of the model.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_json)\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.img_size = img_size\n",
    "        self.pad_values = 114   # Value used for padding in preprocessing.\n",
    "\n",
    "    def __len__(self) -> int: \n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            len(self.img_ids) (int): Number of images.\n",
    "        \"\"\"\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index number.\n",
    "\n",
    "        Returns:\n",
    "            sample (dict): Store image information.\n",
    "        \"\"\"\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs([img_id])[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        org_img = cv2.imread(img_path)\n",
    "        input_img, ratio = self.preprocess(input_img=org_img)   \n",
    "        input_tensor = torch.from_numpy(input_img).unsqueeze(0)\n",
    "\n",
    "        sample = {\n",
    "            'input': input_tensor,\n",
    "            'id': img_id,\n",
    "            'file_name': img_info['file_name'],\n",
    "            'ratio': ratio\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "    def preprocess(self, input_img: np.ndarray) -> Tuple:\n",
    "        \"\"\"\n",
    "        Preprocess an input image for YOLOX model with reshape and CHW transpose (for PyTorch implementation)\n",
    "\n",
    "        Args:\n",
    "            input_img (np.ndarray): Input image as a NumPy array.\n",
    "\n",
    "        Returns:\n",
    "            padded_img (np.ndarray): Preprocessed image as a NumPy array.\n",
    "            ratio (float): Ratio when resizing image.\n",
    "        \"\"\"\n",
    "        padded_img = np.ones((self.img_size[0], self.img_size[1], 3), dtype=np.uint8) * self.pad_values\n",
    "        ratio = min(self.img_size[0] / input_img.shape[0], self.img_size[1] / input_img.shape[1])\n",
    "        resized_img = cv2.resize(input_img, (int(input_img.shape[1] * ratio), int(input_img.shape[0] * ratio)),\n",
    "                                 interpolation=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "        padded_img[: int(input_img.shape[0] * ratio), : int(input_img.shape[1] * ratio)] = resized_img\n",
    "\n",
    "        padded_img = padded_img.transpose((2, 0, 1))\n",
    "        padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "        return padded_img, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CocoDataset(\n",
    "    img_dir = COCO_VAL_IMG_DIR, ann_json = COCO_VAL_ANN_JSON,\n",
    "    img_size = (IMG_HEIGHT, IMG_WIDTH)\n",
    ")\n",
    "calib_dataset = CocoDataset(\n",
    "    img_dir = COCO_TRAIN_IMG_DIR, ann_json=COCO_TRAIN_ANN_JSON,\n",
    "    img_size = (IMG_HEIGHT, IMG_WIDTH)\n",
    ")\n",
    "\n",
    "# For evaluation (batch size 1)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, collate_fn=lambda x: x[0]\n",
    ")\n",
    "# For calibration（No label required）\n",
    "calib_loader = DataLoader(\n",
    "    calib_dataset, batch_size=1, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, collate_fn=lambda x: x[0]\n",
    ")\n",
    "\n",
    "print(len(calib_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2915a6",
   "metadata": {},
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen():\n",
    "    for sample in itertools.islice(itertools.cycle(calib_loader), CALIB_ITER):\n",
    "        yield [sample['input']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea063a",
   "metadata": {},
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://sonysemiconductorsolutions.github.io/mct-model-optimization/api/api_docs/modules/target_platform_capabilities.html)). Here, we use the default Pytorch TPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f5715",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpc = mct.get_target_platform_capabilities('pytorch', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d232bb",
   "metadata": {},
   "source": [
    "## Mixed Precision Configurations\n",
    "We will create a `MixedPrecisionQuantizationConfig` that defines the search options for mixed-precision:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25783c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = mct.core.CoreConfig(\n",
    "    mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(num_of_images=CALIB_ITER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Resource Utilization information to constraint your model's memory size.\n",
    "resource_utilization_data = mct.core.pytorch_resource_utilization_data(\n",
    "    full_float_model,\n",
    "    representative_dataset_gen,\n",
    "    configuration,\n",
    "    target_platform_capabilities=tpc)\n",
    " \n",
    "# Define target Resource Utilization for mixed precision weights quantization.\n",
    "resource_utilization = mct.core.ResourceUtilization(resource_utilization_data.weights_memory * WEIGHTS_COMPRESSION_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6fcf4d",
   "metadata": {},
   "source": [
    "# Post-Training Quantization using MCT\n",
    "Now for the exciting part! Let's run PTQ on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c7f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "                                        in_module=full_float_model,\n",
    "                                        representative_data_gen=representative_dataset_gen,\n",
    "                                        target_platform_capabilities=tpc,\n",
    "                                        core_config=configuration,\n",
    "                                        target_resource_utilization=resource_utilization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773b178",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Now, we will create a function for evaluating a model.  \n",
    "The inference results before and after quantization are displayed on the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model: torch.nn.Module, val_dataloader: DataLoader,\n",
    "             score_threshold: float = 0.1):\n",
    "    \"\"\"\n",
    "    Evaluation of the COCO dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Evaluation model.\n",
    "        val_dataloader (DataLoader): Evaluation dataset.\n",
    "        score_threshold (float): Score threshold.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    class_ids = sorted(val_dataset.coco.getCatIds())\n",
    "    results = []\n",
    "    for sample in tqdm(val_dataloader, desc=\"Evaluating\"):\n",
    "        input_img = sample['input'].to(device)\n",
    "        img_id = sample['id']\n",
    "        ratio = sample['ratio']\n",
    "        boxes, scores, labels = model(input_img)\n",
    "\n",
    "        # boxes: [N, 4] (ymin, xmin, ymax, xmax), scores: [N], labels: [N]\n",
    "        for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "            if score > score_threshold:\n",
    "                box /= ratio\n",
    "                # FasterRCNNBoxDecode return the data of (y, x).\n",
    "                y_min, x_min, y_max, x_max = box.tolist()\n",
    "                width = x_max - x_min\n",
    "                height = y_max - y_min\n",
    "                result = {\n",
    "                    'image_id': img_id,\n",
    "                    'category_id': class_ids[int(label)],\n",
    "                    'bbox': [x_min, y_min, width, height],\n",
    "                    'score': float(score),\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "    # evaluation\n",
    "    coco_gt = val_dataset.coco\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(results)\n",
    "    evaluator = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    evaluator.evaluate()\n",
    "    evaluator.accumulate()\n",
    "    evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029eb053",
   "metadata": {},
   "source": [
    "Let's start with the floating-point model evaluation.  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a557583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating float model（COCO mAP）...\")\n",
    "evaluate(full_float_model, val_dataloader,\n",
    "         score_threshold = SCORE_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef5d0b",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the quantized model:  \n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba05924",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluating quantized model（COCO mAP）...\")\n",
    "evaluate(quantized_model, val_dataloader,\n",
    "         score_threshold = SCORE_THR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f29a",
   "metadata": {},
   "source": [
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Solutions, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-yoloxtest2 (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
